{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e9d7bd-a61a-465d-af9b-8e3c75191423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp37-cp37m-win_amd64.whl (2255.6 MB)\n",
      "     ---------------------------------------- 2.3/2.3 GB 409.6 kB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp37-cp37m-win_amd64.whl (4.8 MB)\n",
      "     ---------------------------------------- 4.8/4.8 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp37-cp37m-win_amd64.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1de5bd0-b185-4ce8-b767-74005ed71461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e20e97-1b2f-4651-9d85-9ac8b65fb80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e8e694c-bd9a-4faa-b92d-1f44a190825e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Laptop GPU'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0440f414-203d-4ec2-a62a-0a3f372e6efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cce550ee-11f2-4005-ac33-01169d7cca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115384, 4)\n",
      "(49617, 3)\n",
      "(49617, 49617)\n",
      "torch.Size([2, 496170])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,    1,    2,    3,    4, 1064, 2135, 1065, 2134,    5]])\n",
      "[[1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]\n",
      "torch.Size([49617, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "\n",
    "# Read the .bin file\n",
    "pc_bin = np.fromfile('./Kitti_dataset/training/velodyne/000000.bin', '<f4')\n",
    "pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "print(pc_bin.shape)  # Should print (125086, 4)\n",
    "pc_bin= pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "# pc_bin\n",
    "# Use the first 3 columns as node features (XYZ coordinates)\n",
    "features = pc_bin[:, :3]\n",
    "print(features.shape)  \n",
    "\n",
    "# Create a k-nearest neighbor graph (e.g., k=10)\n",
    "k = 10\n",
    "adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "print(adj_matrix.shape)\n",
    "adj_matri=adj_matrix[:10, :10]\n",
    "print(edge_index.shape)\n",
    "edge_inde=edge_index[:,:10]\n",
    "print(edge_inde)\n",
    "# print(adj_matrix[:10,:10])\n",
    "adj_matri = adj_matri.toarray() if hasattr(adj_matri, 'toarray') else adj_matri\n",
    "\n",
    "# Print first 10 rows and 10 columns of adj_matrix\n",
    "print(adj_matri[:10, :10])\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "print(x.shape)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Define model\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 16\n",
    "mlp_hidden = 32\n",
    "num_clusters = 5\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Forward pass\n",
    "S = model(data)\n",
    "\n",
    "# Example adjacency matrix (identity for simplicity)\n",
    "A = torch.eye(features.shape[0]).to(device)\n",
    "\n",
    "# Calculate loss\n",
    "loss = model.loss(A, S)\n",
    "print('Loss:', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78c888c-d7ed-405c-a01c-071f95ec3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (2.0.1+cu117)\n",
      "Requirement already satisfied: torchvision in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (0.15.2+cu117)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (2.0.2+cu117)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torchvision) (2.32.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from requests->torchvision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from requests->torchvision) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from requests->torchvision) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9251bedf-309d-4549-a148-b59f02f540ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp311-cp311-win_amd64.whl (500 kB)\n",
      "     ---------------------------------------- 0.0/500.8 kB ? eta -:--:--\n",
      "     -------- ----------------------------- 112.6/500.8 kB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 307.2/500.8 kB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 500.8/500.8 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.2 MB 87.5 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.0/1.2 MB 122.9 kB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.0/1.2 MB 122.9 kB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.0/1.2 MB 122.9 kB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.0/1.2 MB 122.9 kB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.1/1.2 MB 175.0 kB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.1/1.2 MB 175.0 kB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.1/1.2 MB 175.0 kB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.1/1.2 MB 175.0 kB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.1/1.2 MB 175.0 kB/s eta 0:00:07\n",
      "     ------ --------------------------------- 0.2/1.2 MB 276.8 kB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.2/1.2 MB 276.8 kB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.2/1.2 MB 276.8 kB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.2/1.2 MB 276.8 kB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.2/1.2 MB 276.8 kB/s eta 0:00:04\n",
      "     -------------- ------------------------- 0.4/1.2 MB 466.5 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.4/1.2 MB 466.5 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.4/1.2 MB 466.5 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.4/1.2 MB 466.5 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.4/1.2 MB 466.5 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 0.9/1.2 MB 733.9 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.9/1.2 MB 750.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.9/1.2 MB 750.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.9/1.2 MB 750.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 0.9/1.2 MB 750.5 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 886.7 kB/s eta 0:00:00\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp311-cp311-win_amd64.whl (884 kB)\n",
      "     ---------------------------------------- 0.0/884.3 kB ? eta -:--:--\n",
      "     --------- ---------------------------- 225.3/884.3 kB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  880.6/884.3 kB 9.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 884.3/884.3 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting torch-spline-conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt20cpu-cp311-cp311-win_amd64.whl (258 kB)\n",
      "     ---------------------------------------- 0.0/258.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 258.9/258.9 kB 8.0 MB/s eta 0:00:00\n",
      "Collecting scipy (from torch-sparse)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.6 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.6 kB 640.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 60.6/60.6 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from scipy->torch-sparse) (1.26.3)\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/46.2 MB 4.4 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.8/46.2 MB 8.9 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.5/46.2 MB 10.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.5/46.2 MB 13.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 3.5/46.2 MB 14.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 4.5/46.2 MB 16.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 5.0/46.2 MB 16.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.9/46.2 MB 16.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 6.5/46.2 MB 16.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 7.2/46.2 MB 15.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 8.0/46.2 MB 16.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 8.8/46.2 MB 16.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 9.7/46.2 MB 16.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 10.5/46.2 MB 17.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 11.3/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 12.1/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 12.9/46.2 MB 17.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 13.6/46.2 MB 16.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.2/46.2 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.1/46.2 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 15.9/46.2 MB 16.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/46.2 MB 17.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.7/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 18.7/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 19.4/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 20.4/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 21.3/46.2 MB 17.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 22.2/46.2 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 23.1/46.2 MB 18.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 24.1/46.2 MB 18.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 24.9/46.2 MB 19.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 25.9/46.2 MB 19.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 26.8/46.2 MB 19.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 27.6/46.2 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 28.3/46.2 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.2/46.2 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.8/46.2 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 31.0/46.2 MB 19.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 31.7/46.2 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 32.6/46.2 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.5/46.2 MB 18.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 34.3/46.2 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 34.9/46.2 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 35.4/46.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.3/46.2 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 36.9/46.2 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.9/46.2 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 39.0/46.2 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 39.8/46.2 MB 17.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.7/46.2 MB 17.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 41.3/46.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.3/46.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.0/46.2 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.7/46.2 MB 16.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.7/46.2 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.6/46.2 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.2/46.2 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 16.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torch-spline-conv, torch-scatter, scipy, torch-sparse, torch-cluster\n",
      "Successfully installed scipy-1.13.1 torch-cluster-1.6.3+pt20cpu torch-scatter-2.1.2+pt20cpu torch-sparse-0.6.18+pt20cpu torch-spline-conv-1.2.2+pt20cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d5fc61-84c5-4538-a3fc-4f581122cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Using cached torch_geometric-2.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch-geometric) (4.66.4)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch-geometric) (3.1.2)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch-geometric) (2.28.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch-geometric) (5.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from torch-geometric) (1.21.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torch-geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from requests->torch-geometric) (1.26.14)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\heman\\anaconda3\\envs\\gnn\\lib\\site-packages (from tqdm->torch-geometric) (0.4.6)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, torch-geometric\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.1.0 torch-geometric-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install torch-geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa8b00b-8a8e-4aa2-ba3b-82c2c73e6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e06e1f7-857a-4b62-a70e-d841ddabf478",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tensor_cpu \u001b[38;5;241m=\u001b[39m tensor_gpu\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_cpu = tensor_gpu.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3ad395-6302-4547-ad25-8a8d4b1867d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings from torch_geometric\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*torch_geometric.contrib.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*global config object.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*pytorch_lightning.*\")\n",
    "\n",
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    # Delete all tensors\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "    gc.collect()  # Run the garbage collector\n",
    "    torch.cuda.empty_cache()  # Clear the cache\n",
    "\n",
    "# Example usage of the function\n",
    "clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4a759a-2228-4a3e-97b9-06c01eb1e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.3.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from yacs) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from pytorch_lightning) (1.26.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from pytorch_lightning) (2.0.1+cu117)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from pytorch_lightning) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
      "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from pytorch_lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from pytorch_lightning) (4.11.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch_lightning)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (69.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch>=2.0.0->pytorch_lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch>=2.0.0->pytorch_lightning) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch>=2.0.0->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from torch>=2.0.0->pytorch_lightning) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from tqdm>=4.57.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from jinja2->torch>=2.0.0->pytorch_lightning) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from sympy->torch>=2.0.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\heman\\anaconda3\\envs\\re\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n",
      "   ---------------------------------------- 0.0/812.2 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 92.2/812.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 358.4/812.2 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  809.0/812.2 kB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 812.2/812.2 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
      "   ---------------------------------------- 0.0/868.8 kB ? eta -:--:--\n",
      "   ----------------------------------- --- 798.7/868.8 kB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 868.8/868.8 kB 13.8 MB/s eta 0:00:00\n",
      "Installing collected packages: yacs, lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.11.2 pytorch_lightning-2.3.0 torchmetrics-1.4.0.post0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install yacs pytorch_lightning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818fab29-6698-414e-8a82-395aa0706fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44456, 3)\n",
      "torch.Size([2, 444560])\n",
      "Epoch [2/6], Loss: -0.0035973787307739258\n",
      "Epoch [4/6], Loss: -0.01441127061843872\n",
      "Epoch [6/6], Loss: -0.02612370252609253\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 1] <= 10)]\n",
    "\n",
    "    # Extract features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 16\n",
    "mlp_hidden = 32\n",
    "num_clusters = 5\n",
    "# device = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')  # Use CPU for training\n",
    "\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 6\n",
    "print_every = 2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3dad5eb-044c-45a9-94d6-578244956f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49617, 3)\n",
      "torch.Size([2, 496170])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9847386756 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22076\\968299789.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;31m# Example adjacency matrix (identity for simplicity)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;31m# Calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9847386756 bytes."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "import pptk  # Make sure pptk is installed and import it\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "\n",
    "    # Extract features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features, pc_bin\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features, pc_bin = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 16\n",
    "mlp_hidden = 32\n",
    "num_clusters = 5\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')  # Use CPU for training\n",
    "\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1\n",
    "print_every = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Get the cluster assignments\n",
    "_, cluster_assignments = torch.max(S, dim=1)\n",
    "cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "\n",
    "# Visualize segmented data using pptk\n",
    "viewer = pptk.viewer(pc_bin[:, :3])\n",
    "viewer.attributes(cluster_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83033a9b-a2b5-41a1-9f97-b51c06a65612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43910, 3)\n",
      "torch.Size([2, 439100])\n",
      "Epoch [1/3], Loss: -0.036051273345947266\n",
      "Epoch [2/3], Loss: -0.04254782199859619\n",
      "Epoch [3/3], Loss: -0.049385011196136475\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "import pptk  # Make sure pptk is installed and import it\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.3) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "\n",
    "    # Extract features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features, pc_bin\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features, pc_bin = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 16\n",
    "mlp_hidden = 32\n",
    "num_clusters = 5  # Number of clusters set to 4\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')  # Use CPU for training\n",
    "\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "print_every = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Get the cluster assignments\n",
    "_, cluster_assignments = torch.max(S, dim=1)\n",
    "cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "\n",
    "# Visualize segmented data using pptk\n",
    "viewer = pptk.viewer(pc_bin[:, :3])\n",
    "viewer.attributes(cluster_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9f7eef-f0f6-4303-8479-848e06298107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49617, 3)\n",
      "torch.Size([2, 496170])\n",
      "Epoch [1/10], Loss: 0.14600837230682373\n",
      "Epoch [2/10], Loss: 0.12640607357025146\n",
      "Epoch [3/10], Loss: 0.10568690299987793\n",
      "Epoch [4/10], Loss: 0.08560407161712646\n",
      "Epoch [5/10], Loss: 0.06666553020477295\n",
      "Epoch [6/10], Loss: 0.05053055286407471\n",
      "Epoch [7/10], Loss: 0.03535342216491699\n",
      "Epoch [8/10], Loss: 0.019770145416259766\n",
      "Epoch [9/10], Loss: 0.001522064208984375\n",
      "Epoch [10/10], Loss: -0.014343559741973877\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "import pptk  # Make sure pptk is installed and import it\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "\n",
    "    # Extract features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features, pc_bin\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features, pc_bin = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.5),  # Increase dropout rate\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 32  # Increase hidden units in GCNConv layer\n",
    "mlp_hidden = 64  # Increase hidden units in MLP\n",
    "num_clusters = 15  # Number of clusters\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "device = torch.device('cpu')  # Use CPU for training\n",
    "\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Increase the number of epochs\n",
    "print_every = 1  # Print loss every 10 epochs\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0018)  # Decrease learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Get the cluster assignments\n",
    "_, cluster_assignments = torch.max(S, dim=1)\n",
    "cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "\n",
    "# Visualize segmented data using pptk\n",
    "viewer = pptk.viewer(pc_bin[:, :3])\n",
    "viewer.attributes(cluster_assignments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091b7e5-3172-4a3c-aec9-0233c2315498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49617, 3)\n",
      "torch.Size([2, 496170])\n",
      "Epoch [1/10], Loss: 0.2015918493270874\n",
      "Epoch [2/10], Loss: 0.17692971229553223\n",
      "Epoch [3/10], Loss: 0.15489482879638672\n",
      "Epoch [4/10], Loss: 0.13227438926696777\n",
      "Epoch [5/10], Loss: 0.10513544082641602\n",
      "Epoch [6/10], Loss: 0.08331024646759033\n",
      "Epoch [7/10], Loss: 0.07029008865356445\n",
      "Epoch [8/10], Loss: 0.061049818992614746\n",
      "Epoch [9/10], Loss: 0.047197699546813965\n",
      "Epoch [10/10], Loss: 0.031713247299194336\n",
      "Training finished.\n",
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "import pptk  # Make sure pptk is installed and import it\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "\n",
    "    # Extract features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features, pc_bin\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features, pc_bin = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.5),  # Increase dropout rate\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 64  # Increase hidden units in GCNConv layer\n",
    "mlp_hidden = 128  # Increase hidden units in MLP\n",
    "num_clusters = 20  # Decrease number of clusters\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "device = torch.device('cpu')  # Use CPU for training\n",
    "\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Increase the number of epochs\n",
    "print_every = 1  # Print loss every 10 epochs\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0018)  # Decrease learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Get the cluster assignments\n",
    "_, cluster_assignments = torch.max(S, dim=1)\n",
    "cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "\n",
    "# Visualize segmented data using pptk\n",
    "def visualize_clusters(pc_bin, cluster_assignments):\n",
    "    num_clusters = np.max(cluster_assignments) + 1\n",
    "    for i in range(num_clusters):\n",
    "        mask = cluster_assignments == i\n",
    "        viewer = pptk.viewer(pc_bin[mask, :3])\n",
    "        viewer.set(point_size=0.01)\n",
    "        print(i)\n",
    "        input(\"Press Enter to view the next cluster...\")\n",
    "\n",
    "visualize_clusters(pc_bin, cluster_assignments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2099631-07ce-4f33-a55b-0931708a2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44456, 3)\n",
      "torch.Size([2, 444560])\n",
      "Epoch [10/50], Loss: 0.03925466537475586\n",
      "Epoch [20/50], Loss: 0.03351902961730957\n",
      "Epoch [30/50], Loss: 0.02555227279663086\n",
      "Epoch [40/50], Loss: 0.02368175983428955\n",
      "Epoch [50/50], Loss: 0.022513866424560547\n",
      "Training finished.\n",
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to view the next cluster... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "import pptk\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 1] <= 10)]\n",
    "\n",
    "    # Normalize features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features, pc_bin\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features, pc_bin = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layer\n",
    "        self.convs = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.5),  # Increase dropout rate\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.convs(x, edge_index)  # applying convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 64  # Increase hidden units in GCNConv layer\n",
    "mlp_hidden = 128  # Increase hidden units in MLP\n",
    "num_clusters = 5  # Decrease number of clusters\n",
    "device = torch.device('cpu')  # Use CPU for training\n",
    "\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50  # Increase the number of epochs\n",
    "print_every = 10  # Print loss every 10 epochs\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Decrease learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Get the cluster assignments\n",
    "_, cluster_assignments = torch.max(S, dim=1)\n",
    "cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "\n",
    "# Visualize segmented data using pptk\n",
    "def visualize_clusters(pc_bin, cluster_assignments):\n",
    "    num_clusters = np.max(cluster_assignments) + 1\n",
    "    for i in range(num_clusters):\n",
    "        mask = cluster_assignments == i\n",
    "        viewer = pptk.viewer(pc_bin[mask, :3])\n",
    "        viewer.set(point_size=0.01)\n",
    "        print(i)\n",
    "        input(\"Press Enter to view the next cluster...\")\n",
    "\n",
    "visualize_clusters(pc_bin, cluster_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4156c5-30c7-4a59-abcc-384c1960d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch import nn\n",
    "import pptk\n",
    "\n",
    "# Function to load and preprocess point cloud data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Read the .bin file\n",
    "    pc_bin = np.fromfile(file_path, '<f4')\n",
    "    pc_bin = np.reshape(pc_bin, (-1, 4))\n",
    "\n",
    "    # Filter based on Z-axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 2] >= -1.5) & (pc_bin[:, 2] <= 1.5)]\n",
    "\n",
    "    # Further filter based on X and Y axis range\n",
    "    pc_bin = pc_bin[(pc_bin[:, 0] >= -10) & (pc_bin[:, 0] <= 10)]\n",
    "    pc_bin = pc_bin[(pc_bin[:, 1] >= -10) & (pc_bin[:, 1] <= 10)]\n",
    "\n",
    "    # Normalize features (X, Y, Z coordinates)\n",
    "    features = pc_bin[:, :3]\n",
    "    features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "    print(features.shape)  # Print shape of features\n",
    "\n",
    "    return features, pc_bin\n",
    "\n",
    "# Function to create k-nearest neighbor graph and edge index\n",
    "def create_knn_graph(features, k=10):\n",
    "    # Create k-nearest neighbor graph\n",
    "    adj_matrix = kneighbors_graph(features, k, mode='connectivity', include_self=True)\n",
    "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "\n",
    "    return edge_index\n",
    "\n",
    "# Load and preprocess point cloud data\n",
    "file_path = './Kitti_dataset/training/velodyne/000000.bin'\n",
    "features, pc_bin = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Create edge index for k-nearest neighbor graph\n",
    "edge_index = create_knn_graph(features, k=10)\n",
    "print(edge_index.shape)  # Print shape of edge index\n",
    "\n",
    "# Convert features to torch tensor\n",
    "x = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Define GNN model class with a more complex architecture\n",
    "class GNNpool(nn.Module):\n",
    "    def __init__(self, input_dim, conv_hidden, mlp_hidden, num_clusters, device):\n",
    "        super(GNNpool, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_clusters = num_clusters\n",
    "        self.mlp_hidden = mlp_hidden\n",
    "\n",
    "        # GNN conv layers\n",
    "        self.conv1 = pyg_nn.GCNConv(input_dim, conv_hidden)\n",
    "        self.conv2 = pyg_nn.GCNConv(conv_hidden, conv_hidden)\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(conv_hidden, mlp_hidden), \n",
    "            nn.ELU(), \n",
    "            nn.Dropout(0.5),  # Increase dropout rate\n",
    "            nn.Linear(mlp_hidden, self.num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)  # Applying first convolution\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, edge_index)  # Applying second convolution\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # Pass features through MLP\n",
    "        H = self.mlp(x)\n",
    "        # Cluster assignment for matrix S\n",
    "        S = F.softmax(H, dim=1)\n",
    "\n",
    "        return S\n",
    "\n",
    "    def loss(self, A, S):\n",
    "        # Cut loss\n",
    "        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n",
    "        num = torch.trace(A_pool)\n",
    "\n",
    "        D = torch.diag(torch.sum(A, dim=-1))\n",
    "        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n",
    "        den = torch.trace(D_pooled)\n",
    "        mincut_loss = -(num / den)\n",
    "\n",
    "        # Orthogonality loss\n",
    "        St_S = torch.matmul(S.t(), S)\n",
    "        I_S = torch.eye(self.num_clusters, device=self.device)\n",
    "        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n",
    "\n",
    "        return mincut_loss + ortho_loss\n",
    "\n",
    "# Training parameters\n",
    "input_dim = 3  # (x, y, z)\n",
    "conv_hidden = 64  # Increase hidden units in GCNConv layer\n",
    "mlp_hidden = 128  # Increase hidden units in MLP\n",
    "num_clusters = 15  # Number of clusters\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
    "device = torch.device('cpu')\n",
    "# Instantiate model\n",
    "model = GNNpool(input_dim, conv_hidden, mlp_hidden, num_clusters, device).to(device)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50  # Increase the number of epochs\n",
    "print_every = 10  # Print loss every 10 epochs\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Decrease learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    S = model(data)\n",
    "    \n",
    "    # Example adjacency matrix (identity for simplicity)\n",
    "    A = torch.eye(features.shape[0]).to(device)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = model.loss(A, S)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print loss every print_every epochs\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Get the cluster assignments\n",
    "_, cluster_assignments = torch.max(S, dim=1)\n",
    "cluster_assignments = cluster_assignments.cpu().numpy()\n",
    "\n",
    "# Visualize segmented data using pptk\n",
    "def visualize_clusters(pc_bin, cluster_assignments):\n",
    "    num_clusters = np.max(cluster_assignments) + 1\n",
    "    for i in range(num_clusters):\n",
    "        mask = cluster_assignments == i\n",
    "        viewer = pptk.viewer(pc_bin[mask, :3])\n",
    "        viewer.set(point_size=0.01)\n",
    "        print(i)\n",
    "        input(\"Press Enter to view the next cluster...\")\n",
    "\n",
    "visualize_clusters(pc_bin, cluster_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705847a7-fa70-4168-8f19-b4ec2a1356e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate Intersection over Union (IoU)\n",
    "def calculate_iou(predicted_labels, ground_truth_labels):\n",
    "    unique_labels = np.unique(ground_truth_labels)\n",
    "    ious = []\n",
    "    for label in unique_labels:\n",
    "        pred_mask = (predicted_labels == label)\n",
    "        gt_mask = (ground_truth_labels == label)\n",
    "        intersection = np.sum(pred_mask & gt_mask)\n",
    "        union = np.sum(pred_mask | gt_mask)\n",
    "        if union == 0:\n",
    "            iou = 1.0  # Perfect match\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        ious.append(iou)\n",
    "    return np.mean(ious)\n",
    "\n",
    "# Function to calculate Adjusted Rand Index (ARI)\n",
    "def calculate_ari(predicted_labels, ground_truth_labels):\n",
    "    ari = adjusted_rand_score(ground_truth_labels, predicted_labels)\n",
    "    return ari\n",
    "\n",
    "# Assuming you have ground truth labels for comparison\n",
    "ground_truth_labels = ...  # Load or define your ground truth labels here\n",
    "detected_labels = cluster_assignments  # Detected cluster assignments from your model\n",
    "\n",
    "# Calculate IoU and ARI\n",
    "iou = calculate_iou(detected_labels, ground_truth_labels)\n",
    "ari = calculate_ari(detected_labels, ground_truth_labels)\n",
    "\n",
    "print(f'Intersection over Union (IoU): {iou}')\n",
    "print(f'Adjusted Rand Index (ARI): {ari}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
